{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5575fad4",
   "metadata": {},
   "source": [
    "# 源数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286db6b0",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71e4ad43",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import PeftModel\n",
    "from transformers import pipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308115ed",
   "metadata": {},
   "source": [
    "en_path = r'H:\\datasets\\data\\翻译1\\test.en.txt'\n",
    "ch_path = r'H:\\datasets\\data\\翻译1\\test.ch.txt'\n",
    "csv_path=r'C:\\Users\\30535\\Desktop'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab20aec",
   "metadata": {},
   "source": [
    "class TextToCsv:\n",
    "    ## 定义tokenizer,对原始数据进行处理\n",
    "    def __init__(self, en_path, ch_path,csv_path,text_pair_nums=50000):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param en_path: 英文数据路径\n",
    "        :param ch_path: 中文数据路径\n",
    "        :csv_path 文件保存路径\n",
    "        :text_pair_nums: 使用多少对数据\n",
    "        \"\"\"\n",
    "        self.en_path = en_path  # 英文路径\n",
    "        self.ch_path = ch_path  # 中文路径\n",
    "        self.text_pair_nums=text_pair_nums\n",
    "        \n",
    "        # 读取原始英文数据\n",
    "        self.en_data = self.__read_ori_data(en_path)\n",
    "        # 读取原始中文数据\n",
    "        self.ch_data = self.__read_ori_data(ch_path)\n",
    "        self.x=self.return_csv(csv_path)\n",
    "\n",
    "    def __read_ori_data(self, path):\n",
    "        \"\"\"\n",
    "        读取原始数据\n",
    "        :param path: 数据路径\n",
    "        :return: 返回一个列表，每个元素是一条数据\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().split('\\n')[:-1]\n",
    "            self.text_pair_nums =self.text_pair_nums if self.text_pair_nums <=len(data) else len(data)\n",
    "            data = data[:self.text_pair_nums] \n",
    "        return data\n",
    "    \n",
    "    def return_csv(self,csv_path):\n",
    "        \"\"\"\n",
    "        将源数据处理成csv文件\n",
    "        :csv_path 文件保存路径\n",
    "        \"\"\"\n",
    "        data=[]\n",
    "        # 遍历所有数据，长度大于127的数据抛弃\n",
    "        for i in range(self.text_pair_nums):\n",
    "            if len(self.en_data[i])>127 or len(self.en_data[i])>127:\n",
    "                continue\n",
    "            # 英文→中文\n",
    "            data.append([\n",
    "                self.en_data[i],\n",
    "                self.ch_data[i]]\n",
    "            )\n",
    "            # 中文→英文\n",
    "            data.append([\n",
    "                self.ch_data[i],\n",
    "                self.en_data[i]]\n",
    "            )\n",
    "        random.shuffle(data) # 数据随机打乱\n",
    "        csv_train=os.path.join(csv_path,'train.csv') # 训练集文件\n",
    "        csv_test=os.path.join(csv_path,'test.csv') # 测试集文件\n",
    "        dat=pd.DataFrame(data[:len(data)-1000],columns=['src','tgt']) # 训练集\n",
    "        dat2=pd.DataFrame(data[len(data)-1000:],columns=['src','tgt']) # 测试集\n",
    "        dat.to_csv(csv_train,index=False) # 转换为csv文件\n",
    "        dat2.to_csv(csv_test,index=False)\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e24831",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "TextToCsv(en_path,ch_path,csv_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5cfcfa66",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dd34940",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "77a540d7",
   "metadata": {},
   "source": [
    "## 2 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d6fc0b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "data_train=r'C:\\Users\\30535\\Desktop\\train.csv'\n",
    "data_test=r'C:\\Users\\30535\\Desktop\\test.csv'\n",
    "ds=load_dataset('csv',data_files={'train':data_train, 'test': data_test},\n",
    "                                split=['train', 'test'])\n",
    "ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d63ae622",
   "metadata": {},
   "source": [
    "## 4 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d71b691",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model_path=r'H:\\models\\bloom-2b5-zh'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f48676e",
   "metadata": {},
   "source": [
    "def process_func(examples):\n",
    "    MAX_LENGTH = 150\n",
    "    contents='机器翻译:\\n' + examples['src']\n",
    "    # 对输入与label进行编码\n",
    "    inputs=tokenizer(contents)\n",
    "    labels = tokenizer(text_target=examples['tgt'] + tokenizer.eos_token)\n",
    "    input_ids=inputs[\"input_ids\"]+labels[\"input_ids\"]\n",
    "    attention_mask=inputs[\"attention_mask\"] + labels[\"attention_mask\"]\n",
    "    labels = [-100] * len(inputs[\"input_ids\"]) + labels[\"input_ids\"]\n",
    "    # 数据截断\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb8f1a88",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "tokenized_train=ds[0].map(process_func, remove_columns=ds[0].column_names)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90825de",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "tokenized_train"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ad20e4d9",
   "metadata": {},
   "source": [
    "## 5 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f5fa333",
   "metadata": {},
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fec97cb",
   "metadata": {},
   "source": [
    "model = model.half()\n",
    "model=model.to('cuda')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320650bf",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3aaa4c6",
   "metadata": {},
   "source": [
    "# 6.1 创建配置文件\n",
    "from peft import LoraConfig,get_peft_model,TaskType\n",
    "comfig = LoraConfig(task_type=TaskType.CAUSAL_LM)\n",
    "comfig"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cc4ec1c",
   "metadata": {},
   "source": [
    "# 6.2 创建模型\n",
    "model_lora = get_peft_model(model,comfig)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4495dfdb",
   "metadata": {},
   "source": [
    "model_lora=model_lora.half()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1938044",
   "metadata": {},
   "source": [
    "x=\"机器翻译:\\n{}\".format(\"what is this。\").strip()\n",
    "ipt = tokenizer(x,return_tensors='pt').to('cuda')\n",
    "print(tokenizer.decode(model.generate(**ipt,max_length=256, do_sample=False)[0],skip_special_tokens=True)[len(x):])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33be677b",
   "metadata": {},
   "source": [
    "model_lora.print_trainable_parameters()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d689a5",
   "metadata": {},
   "source": [
    "## 7 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a2e300",
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\" # 防止日志输出到wandb.ai\n",
    "args= TrainingArguments(\n",
    "                                  output_dir='./modelcheak/m3',\n",
    "                                  logging_dir=r'./modelcheak/m3',\n",
    "                                  per_device_train_batch_size=8,  # batch_size\n",
    "                                  gradient_accumulation_steps=4,\n",
    "                                  logging_steps=20,\n",
    "                                  optim=\"adafactor\",  # 使用特定的优化器优化显存\n",
    "                                  save_strategy='epoch',  # 每一轮保存一个模型\n",
    "                                  num_train_epochs=1,\n",
    "                                  adam_epsilon=1e-4\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "122adaa1",
   "metadata": {},
   "source": [
    "## 8 创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43b7e698",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "trainr=Trainer(\n",
    "    args=args,\n",
    "    model=model_lora,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0943fb9e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "trainr.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7ae220ef",
   "metadata": {},
   "source": [
    "## 9 权重合并与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aca0d3b",
   "metadata": {},
   "source": [
    "from peft import PeftModel\n",
    "# model_id 是checkpoint那个路径\n",
    "prft_model=PeftModel.from_pretrained(model=model,model_id=r\"C:\\Users\\30535\\Desktop\\CodeProgram\\Python\\deepstudy\\code2\\使用Transformer进行中英文翻译\\modelcheak\\m3\\checkpoint-2895\")\n",
    "# 权重合并\n",
    "merge_model=prft_model.merge_and_unload()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fc687a",
   "metadata": {},
   "source": [
    "# 模型保存\n",
    "merge_model.save_pretrained('./modelcheak/trans11')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "876ab7dc",
   "metadata": {},
   "source": [
    "x=\"机器翻译:\\n{}\".format(\"what is this。\").strip()\n",
    "ipt = tokenizer(x,return_tensors='pt').to('cuda')\n",
    "print(tokenizer.decode(merge_model.generate(**ipt,max_length=256, do_sample=False)[0],skip_special_tokens=True)[len(x):])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae2b6ec1",
   "metadata": {},
   "source": [
    "x=\"机器翻译:\\n{}\".format(\"这又是什么呢？\").strip()\n",
    "ipt = tokenizer(x,return_tensors='pt').to('cuda')\n",
    "print(tokenizer.decode(merge_model.generate(**ipt,max_length=256, do_sample=False)[0],skip_special_tokens=True)[len(x):])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce272f6e",
   "metadata": {},
   "source": [
    "import re\n",
    "import sacrebleu\n",
    "def is_english_sentence(sentence):\n",
    "    # 使用正则表达式检查句子中是否包含英文字母\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    match = english_pattern.search(sentence)\n",
    "    \n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "smooth = SmoothingFunction().method1\n",
    "bleu_scores=[]\n",
    "m1,m2=[],[]\n",
    "m3,m4=[],[]\n",
    "import time\n",
    "t=time.time()\n",
    "for i in range(100):\n",
    "    if i%40==0:\n",
    "        print(i/len(ds[1]['src']))\n",
    "    x=\"机器翻译:\\n{}\".format(ds[1]['src'][i]).strip()\n",
    "    ipt = tokenizer(x,return_tensors='pt').to('cuda')\n",
    "    y=tokenizer.decode(merge_model.generate(**ipt,max_length=150, do_sample=False)[0],skip_special_tokens=True)[len(x):]\n",
    "    if is_english_sentence(ds[1]['tgt'][i]):\n",
    "        m1.append(ds[1]['tgt'][i])\n",
    "        m2.append([y])\n",
    "    else:\n",
    "        m3.append(list(ds[1]['tgt'][i][:-1]))\n",
    "        m4.append([list(y)[:-1]])\n",
    "print('时间',time.time()-t)\n",
    "smooth = SmoothingFunction().method1\n",
    "b1=[sacrebleu.sentence_bleu(candidate, refs).score for candidate, refs in zip(m1, m2)]\n",
    "for i in range(len(m4)):\n",
    "    b2 = sentence_bleu(m4[i], m3[i], weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)*100\n",
    "    b1.append(b2)\n",
    "print(sum(b1)/100)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162c152",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9a092afd",
   "metadata": {},
   "source": [
    "## 9 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df9ad78c",
   "metadata": {},
   "source": [
    "from transformers import pipeline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7145468b",
   "metadata": {},
   "source": [
    "pipe=pipeline('text2text-generation',model=merge_model,tokenizer=tokenizer,device=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89d02ec1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pipe('机器翻译:\\n'+'我有一个苹果',max_length=30,do_sample=False)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
